---
title: "Simulation"
author: "Gilles Lijnzaad"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: TRUE
    toc_float:
      collapsed: TRUE
    highlight: tango
    number_sections: true
---

```{r setup, include = FALSE}
knitr::knit_hooks$set(purl = knitr::hook_purl)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.width = 10, fig.height = 4)
```

# The simple model
## Preparation
Defining experimental parameters (i.e., number of participants and number of trials).
```{r prep}
rm(list = ls())
n_participants <- 20
n_trials <- 40
```

## Model definition
### Participant & trial loop
For each participant, I first initialize parameters and then run a number of trials. This fills up a data frame that gets added to the total data `dat`. 
```{r model-std}
library(truncnorm) # draw from a truncated normal distribution (for rating)

runmod <- function(params = params_std) {
  dat <- data.frame()

  for (j in 1:n_participants) {
    set.seed(j)

    # ------ init data frames etc -----
    Q <- data.frame(
      F = rep(NA, n_trials),
      U = rep(NA, n_trials)
    )
    P_F <- c()
    choice <- c()
    R <- c()
    pred_err <- c()

    # ----- initialize parameters -----
    LR <- params[1]
    inv_temp <- params[2]
    Q$F[1] <- params[3]
    Q$U[1] <- params[4]
    mu_R <- params[5:6]
    names(mu_R) <- c("F", "U")
    sigma_R <- params[7]

    # --------- run trials ------------
    for (t in 1:n_trials) {

      # choose
      P_F[t] <- 1 / (1 + exp(-inv_temp * (Q$F[t] - Q$U[t])))
      choice[t] <- if_else(runif(1) < P_F[t],
                           "F",
                           "U")

      # rate
      R[t] <- round(rtruncnorm(n = 1, a = 1, b = 10, 
                        mean = mu_R[[choice[t]]], 
                        sd = sigma_R),
                    0)

      # learn
      pred_err[t] <- R[t] - Q[t, choice[t]]

      if (t < n_trials) {   # no updating Qs in the very last trial
        Q[t+1, choice[t]] <- Q[t, choice[t]] + LR * pred_err[t]

        not_chosen <- colnames(Q[which(colnames(Q) != choice[t])])
        Q[t+1, not_chosen] <- Q[t, not_chosen]
      }
    }

    dat_p <- data.frame(
      participant = rep(j, n_trials),
      trial =       1:n_trials,
      Q_F =         Q$F,
      Q_U =         Q$U,
      P_F =         P_F,
      choice =      choice,
      R =           R,
      pred_err =    pred_err
    )

    dat <- rbind(dat, dat_p)
  }

  return(dat)
}
```

## Helper functions
### Plots
I want to create plots of Q over time. I define functions for transforming the data to a long format and for plotting, to make plotting quick and easy.

```{r plot-util}
library(tidyverse)
my_teal <- "#008080"
my_pink <- "#ff00dd"

my_theme <- theme_bw() +
  theme(plot.title = element_text(size = 20, face = "bold")) +
  theme(axis.text = element_text(size = 16),
        axis.title = element_text(size = 18)) +
  theme(legend.title = element_blank(),
        legend.text = element_text(size = 16)) +
  theme(strip.text.x = element_text(size = 18, face = "bold"))
```

```{r helper-fun}
library(grid)
library(gridExtra)

to_long <- function(dat) {
  long_dat <- dat %>%
    pivot_longer(c(Q_F, Q_U), names_prefix = "Q_", names_to = "option", values_to = "Q") %>%
    mutate(option = factor(option),
           choice = factor(choice))

  return(long_dat)
}

plot_Q <- function(dat) {
  p <- ggplot(dat, aes(x = trial,
                      y = Q,
                      color = option)) +
    geom_smooth(aes(fill = option)) +
    ylim(c(1, 10)) +
    labs(x = "Trial") +
    scale_color_manual(values = c(my_teal, my_pink),
                      labels = c("Friendly", "Unfriendly")) +  
    scale_fill_manual(values = c(my_teal, my_pink),
                      labels = c("Friendly", "Unfriendly")) +
    my_theme +
    theme(legend.position = "inside",
          legend.position.inside = c(0.83, 0.91))

  return(p)
}

plot_choice <- function(dat) {
  dat <- dat %>%
    mutate(choice_is_F = as.numeric(choice == "F"),
           choice_is_U = 1 - choice_is_F)
    
  p <- ggplot(dat, aes(x = trial)) +
    geom_smooth(aes(y = choice_is_F),
                color = paste0(my_teal, "30"),
                fill = paste0(my_teal, "30")) +
    geom_smooth(aes(y = choice_is_U),
                color = paste0(my_pink, "30"),
                fill = paste0(my_pink, "30")) +
    ylim(c(0, 1)) +
    labs(x = "Trial",
         y = "Proportion chosen") +
    my_theme
  return(p)
}

my_annotation <- function(params) {
    text <- paste0("LR = ", params[1],
                  "\ninv_temp = ", params[2],
                  "\ninitQF = ", params[3],
                  "\ninitQU = ", params[4],
                  "\nmu_R_F = ", params[5],
                  "\nmu_R_U = ", params[6],
                  "\nsigma_R = ", params[7])

    grid.text(text, x = unit(0.98, "npc"), y = unit(0.95, "npc"), hjust = 1, vjust = 1)
}
```

### Data for Stan
I want to plug simulated data into Stan for model recovery. To this end, I need to convert the relevant data points to JSON format.
```{r dat-to-JSON}
library(cmdstanr) # contains function write_stan_json()
dir <- "~/research/climate-RL/R_simulation/"

write_sim_dat_JSON <- function(params, model_dat) {
  # parameter settings
  LR <- params[1]
  inv_temp <- params[2]
  initQF <- params[3]
  initQU <- params[4]
  mu_R_F <- params[5]
  mu_R_U <- params[6]
  sigma_R <- params[7]
  T <- n_trials
  n_part <- n_participants
  par_names <- c("LR", "inv_temp", "initQF", "initQU", "mu_R_F", "mu_R_U", "sigma_R", "T", "n_part")
  list_param_settings <- setNames(mget(par_names), par_names)
  write_stan_json(list_param_settings, file = paste0(dir, "sim_param_settings.json"))

  # data
  choice <- matrix(as.numeric(model_dat$choice == "U") + 1,
                   nrow = n_part,
                   ncol = T)
  R <- matrix(model_dat$R,
              nrow = n_part,
              ncol = T)
  dat_names <- c("n_part", "T", "initQF", "initQU", "choice", "R")
  list_dat <- setNames(mget(dat_names), dat_names)
  write_stan_json(list_dat, file = paste0(dir, "sim_dat.json"))
}
```

In order to get a better picture of the data we gave as input to Stan, I want to save a plot of the Q values and of the choices to the modeling directory.
```{r dat-to-rds}
save_plot_dat <- function(params, dat) {
  dat <- dat %>% to_long()
  dir <- "~/research/climate-RL/stan_models/my_modeling/"
  saveRDS(plot_Q(dat), file = paste0(dir, "plot_Q.rds"))
  saveRDS(plot_choice(dat), file = paste0(dir, "plot_choice.rds"))
}
```

## Run & inspect
### Standard settings
```{r run-std}
# generate data for Stan
stan_params <- c(0.6, 1.2, 5, 5, 8, 3, 3) # LR, inv_temp, Q_F_1, Q_U_1, mu_R_F, mu_R_U, sigma_R
stan_dat <- runmod(stan_params)
write_sim_dat_JSON(stan_params, stan_dat)
save_plot_dat(stan_params, stan_dat)

params_std <- c(0.5, 0.5, 8, 3, 5, 5, 3) # LR, inv_temp, Q_F_1, Q_U_1, mu_R_F, mu_R_U, sigma_R
dat_std <- runmod()
dat_std <- dat_std %>% to_long()
p_left <- plot_Q(dat_std)
p_right <- plot_choice(dat_std)
grid.arrange(p_left, p_right, nrow = 1)
my_annotation(params_std)
```

### More extreme initial values
```{r run-std-init-val}
params <- c(params_std[1:2], 10, 1, 5, 5, 3)
dat <- runmod(params)
dat <- dat %>% to_long()
p_left <- plot_Q(dat)
p_right <- plot_choice(dat)
grid.arrange(p_left, p_right, nrow = 1)
my_annotation(params)
```
Even with differential initial values, Q-values quickly converge due to the stochastic nature of the rating function.

### Varying learning rate
The code for this and all other plots is the same as the code in 3.1, just different `params`, so it will not be included.
```{r run-std-LR, echo = FALSE}
params <- c(0.2, params_std[2:7])
dat <- runmod(params) %>% to_long()
p_left <- plot_Q(dat)
p_right <- plot_choice(dat)
grid.arrange(p_left, p_right, nrow = 1)
my_annotation(params)

params <- c(0.8, params_std[2:7])
dat <- runmod(params) %>% to_long()
p_left <- plot_Q(dat)
p_right <- plot_choice(dat)
grid.arrange(p_left, p_right, nrow = 1)
my_annotation(params)
```
A lower learning rate results in a smaller effect of ratings, and therefore less variance in Q values (ribbons are smaller). A higher learning rate does the opposite.

### Varying inverse temperature
```{r run-std-inv-temp, echo = FALSE}
params <- c(params_std[1], 0, params_std[3:7])
dat <- runmod(params) %>% to_long()
p_left <- plot_Q(dat)
p_right <- plot_choice(dat)
grid.arrange(p_left, p_right, nrow = 1)
my_annotation(params)

params <- c(params_std[1], 1.5, params_std[3:7])
dat <- runmod(params) %>% to_long()
p_left <- plot_Q(dat)
p_right <- plot_choice(dat)
grid.arrange(p_left, p_right, nrow = 1)
my_annotation(params)
```
The inverse temperature has a direct effect on the choice probabilities only, and therefore has an *indirect* effect on Q values by determining the option that gets its Q-value updated next trial. 

# Confirmation bias

The [decision tree](../confirmation_bias_overview/confirmation_bias_models_v3.pdf) shows the many options we have for implementing confirmation bias.

The first differentiation is whether bias occurs at the level of learning (LRN) or the level of rating (RTN).

## Learning: LRN
We assume that the relationship between confirmation and learning rate (LR) is either discrete (discr) or continuous (cont).

### Discrete (discr)
Confirmatory and disconfirmatory outcomes have separate learning rates, where LR_conf > LR_disconf. As a standard, we say LR_conf = 0.7 and LR_disconf = 0.3.

### Continuous (cont)

## Rating: RTN