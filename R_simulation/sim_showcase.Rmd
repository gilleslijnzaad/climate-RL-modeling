---
title: "Simulation showcase"
author: "Gilles Lijnzaad"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: TRUE
    toc_float:
      collapsed: TRUE
    highlight: tango
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::knit_hooks$set(purl = knitr::hook_purl)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.width = 10, fig.height = 4)
```

# 0. Simulation code
I define the simulation code in its own file. For ease of reference I include it below as well.
```{r sim-code}
rm(list = ls())
source("sim.R")
temp <- readLines("sim.R")
start <- grep("# === run_sim", temp)
end <- grep("# === end of run_sim", temp) - 1
cat(temp[start:end], sep = "\n")
```

# 1. Standard settings
```{r run-std}
params_std <- list(
  n_part = 50,
  n_trials = 30,
  LR = 0.4,
  inv_temp = 0.5,
  initQF = 5,
  initQU = 5,
  mu_R = c(8, 2), # F and U
  sigma_R = 1
)

dat <- run_sim(params_std)
gridExtra::grid.arrange(plot_Q(dat), plot_choice(dat), nrow = 1)
my_annotation(params_std)
```

# 2. Varying learning rate
```{r run-std-LR}
params <- modifyList(params_std, list(LR = 0.2))
dat <- run_sim(params)
gridExtra::grid.arrange(plot_Q(dat), plot_choice(dat), nrow = 1)
my_annotation(params)

params <- modifyList(params_std, list(LR = 0.8))
dat <- run_sim(params)
gridExtra::grid.arrange(plot_Q(dat), plot_choice(dat), nrow = 1)
my_annotation(params)
```

# 3. Varying inverse temperature
```{r run-std-inv-temp}
params <- modifyList(params_std, list(inv_temp = 0))
dat <- run_sim(params)
gridExtra::grid.arrange(plot_Q(dat), plot_choice(dat), nrow = 1)
my_annotation(params)

params <- modifyList(params_std, list(inv_temp = 1.5))
dat <- run_sim(params)
gridExtra::grid.arrange(plot_Q(dat), plot_choice(dat), nrow = 1)
my_annotation(params)
```

# 4. Confirmation bias

The [decision tree](../confirmation_bias_overview/confirmation_bias_models_v3.pdf) shows the many options we have for implementing confirmation bias.

The first differentiation is whether bias occurs at the level of learning (LRN) or the level of rating (RTN).

## Learning: LRN
We assume that the relationship between confirmation and learning rate (LR) is either discrete (discr) or continuous (cont).

### Discrete (discr)
Confirmatory and disconfirmatory outcomes have separate learning rates, where LR_conf > LR_disconf. As a standard, we say LR_conf = 0.7 and LR_disconf = 0.3.

### Continuous (cont)

## Rating: RTN